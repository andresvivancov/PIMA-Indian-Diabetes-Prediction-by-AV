# -*- coding: utf-8 -*-
"""PIMA-Indian-Diabetes-Prediction-by-AV.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tcF-hCqsY7mBMgKe8dPLJLc79JaeRCtt

# PIMA-Indian-Diabetes-Prediction-by-AV

## 0. Info

From https://github.com/andresvivancov/PIMA-Indian-Diabetes-Prediction-by-AV


Data Sources https://github.com/andresvivancov/

PIMA-Indian-Diabetes-Prediction-by-AV/tree/master/data


Andres Vivanco 


June 2020

## 1. Data Collection & Integration
"""

# Commented out IPython magic to ensure Python compatibility.
# Originally from UCI Machine Learning Repository
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# %matplotlib inline

df = pd.read_csv ('https://raw.githubusercontent.com/andresvivancov/PIMA-Indian-Diabetes-Prediction-by-AV/master/data/pima-data.csv')

"""## 2. Data Preparation"""

df.head(3)

df.describe

df.shape

df.tail(4)

"""Checking null values in df"""

df.isnull().sum()

df.isnull().values.any()

"""###Checking correlation"""

df.corr(method= 'pearson')

df.corr()

"""## 3. Data Visualization & Analysis"""

def plot_correlation (df, size =11):
  corr= df.corr()
  fig, ax = plt.subplots (figsize=(size, size))
  ax.matshow(corr)
  plt.xticks (range(len(corr.columns)), corr.columns)
  plt.yticks (range(len(corr.columns)), corr.columns)

plot_correlation (df)

"""## 4. Feauture Selection & Engineering"""

del df['skin']

df.head(4)

plot_correlation (df)

df.head(5)

#Change True to 1 and False to 0
diabetes_map = {True: 1, False: 0}

df['diabetes'] = df['diabetes'].map(diabetes_map)

df.head(5)

num_true = len(df.loc[df['diabetes'] == True ])
num_false = len (df.loc[df['diabetes'] == False])

#True Cases
print('Number of True Cases in the dataset: {0} ({1:2.2f}%)'.format(num_true, (num_true/(num_true + num_false)) * 100  ))

#False Cases
print('Number of False Cases in the dataset: {0} ({1:2.2f}%)'.format(num_false, (num_false/(num_true + num_false)) * 100  ))



"""## 5. Model Training

**Algorithm Selected:**
  Naive Bayes
**Type:**
Bynari Classification

Splitting Data in training and testing


70% Training
30% Testing
"""

import numpy as np
from sklearn.model_selection import train_test_split

df.columns

feature_columns = ['num_preg','glucose_conc','diastolic_bp','thickness','insulin','bmi','diab_pred','age','diabetes']
predicted_class_names = ['diabetes']

X = df[feature_columns].values
y = df[predicted_class_names].values # predicted class

#dividimos 70 / 30
split_test_size = 0.30

X_train, X_test, y_train, y_test = train_test_split ( X, y, test_size = split_test_size, random_state = 33 )

"""Review if it is 70 % for training and 30 % of testing"""

print('Splitting')
print("{0:0.2f}% in training dataset".format((len(X_train)/len(df.index))*100))
print("{0:0.2f}% in training dataset".format((len(X_test)/len(df.index))*100))

"""Let's check the distribution of true en false cases"""

print('Original True : {0}  ({1:0.2f}%)'.format(len(df.loc[df['diabetes'] == 1]),   (len(df.loc[df['diabetes'] == 1]) / len(df.index))  *100))
print('Original False : {0}  ({1:0.2f}%)'.format(len(df.loc[df['diabetes'] == 0]),   (len(df.loc[df['diabetes'] == 0]) / len(df.index)) *100))
print('Original Total : {0}  ({1:0.2f}%)'.format(len(df.loc[df['diabetes'] ]),   (len(df.loc[df['diabetes'] ]) / len(df.index)) *100))
print('')
print('Training True : {0}  ({1:0.2f}%)'.format(len(y_train[y_train[:] == 1]),   (len(y_train[y_train[:] == 1]) / len(y_train) * 100)))
print('Training False : {0}  ({1:0.2f}%)'.format(len(y_train[y_train[:] == 0]),   (len(y_train[y_train[:] == 0]) / len(y_train) * 100)))
print('Training Total : {0}  ({1:0.2f}%)'.format(len(y_train[y_train[:] ]),   (len(y_train[y_train[:] ]) / len(y_train) * 100)))
print('')
print('Test True : {0}  ({1:0.2f}%)'.format(len(y_test[y_test[:] == 1]),   (len(y_test[y_test[:] == 1]) / len(y_test) * 100)))
print('Test False : {0}  ({1:0.2f}%)'.format(len(y_test[y_test[:] == 0]),   (len(y_test[y_test[:] == 0]) / len(y_test) * 100)))
print('Test Total : {0}  ({1:0.2f}%)'.format(len(y_test[y_test[:] ]),   (len(y_test[y_test[:] ]) / len(y_test) * 100)))

"""Post-split Data Preparation"""

df.head()

df.shape

"""Analyzing zero values"""

df.isin([0]).sum()
#np.count_nonzero(df)

"""**Impute with the mean**"""

from sklearn.impute import SimpleImputer


imputer = SimpleImputer(missing_values=0, strategy='mean')

X_train = imputer.fit_transform(X_train)
X_test = imputer.fit_transform(X_test)

"""Training Initial Algorithm Naive Bayes"""

from sklearn.naive_bayes import GaussianNB

#Creating the model
nb_model = GaussianNB()

nb_model.fit(X_train, y_train.ravel())











"""## 6. Model Evaluation

**Performance on Training Data**
"""

#Predicting values using the training dataset
nb_predict_train = nb_model.predict(X_train)

#import performance metrics library
from sklearn import metrics

#Check Accurary
print('Accurary: {0:.4f}'.format (metrics.accuracy_score(y_train, nb_predict_train)))

"""**Performance on Testing Data**"""

#Predicting values using the testing dataset
nb_predict_test = nb_model.predict(X_test)

#import performance metrics library
from sklearn import metrics

#Check Accurary
print('Accurary: {0:.4f}'.format (metrics.accuracy_score(y_test, nb_predict_test)))

"""**Confusion Matrix**"""

print("Confusion Matrix")
print("{0}".format(metrics.confusion_matrix(y_test, nb_predict_test)))
print('')
print('TN    FP')
print('FN    TP')
# TN FP
#FN  TP

print('Classification Report')
print(metrics.classification_report(y_test, nb_predict_test))

"""### Random Forest

**Trying another algorith: Random Forest**

![alt text](https://miro.medium.com/max/592/1*i0o8mjFfCn-uD79-F1Cqkw.png)
"""

from sklearn.ensemble import RandomForestClassifier

rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train, y_train.ravel())

"""Predicting training dataset"""

#Predicting values using the training dataset
rf_predict_train = rf_model.predict(X_train)

#import performance metrics library
from sklearn import metrics

#Check Accurary
print('Accurary: {0:.4f}'.format (metrics.accuracy_score(y_train, rf_predict_train)))

"""Predicting test dataset"""

#Predicting values using the testing dataset
rf_predict_test = nb_model.predict(X_test)


#Check Accurary
print('Accurary: {0:.4f}'.format (metrics.accuracy_score(y_test, rf_predict_test)))

print("Confusion Matrix")
print("{0}".format(metrics.confusion_matrix(y_test, rf_predict_test)))
print('')
print('TN    FP')
print('FN    TP')
# TN FP
#FN  TP

print('Classification Report')
print(metrics.classification_report(y_test, rf_predict_test))

"""#### Here we have overfitting with Random Forest

#### **Logistic Regression**
"""

from sklearn.linear_model import LogisticRegression

logreg = LogisticRegression(max_iter=1000)

lr_model = LogisticRegression(C=0.7, random_state=42, max_iter=1000)
lr_model.fit(X_train, y_train.ravel())
lr_predict_test = lr_model.predict(X_test)

#training metrics


#Check Accurary
print('Accurary: {0:.4f}'.format (metrics.accuracy_score(y_test, lr_predict_test)))

print("Confusion Matrix")
print("{0}".format(metrics.confusion_matrix(y_test, lr_predict_test)))
print('')
print('TN    FP')
print('FN    TP')
# TN FP
#FN  TP

print('Classification Report')
print(metrics.classification_report(y_test, lr_predict_test))

"""**Setting Regularization parameter**"""

# Commented out IPython magic to ensure Python compatibility.
C_start = 0.1
C_end = 5
C_inc = 0.1

C_values, recall_scores = [], []

C_val = C_start
best_recall_score = 0
while(C_val < C_end):
    C_values.append(C_val)
    lr_model_loop = LogisticRegression (C=C_val, random_state=42,solver='liblinear', max_iter=1000000)
    lr_model_loop.fit(X_train, y_train.ravel())
    lr_predict_loop_test = lr_model_loop.predict(X_test)
    recall_score = metrics.recall_score(y_test, lr_predict_loop_test)
    recall_scores.append(recall_score)
    if(recall_score > best_recall_score):
      best_recall_score = recall_score
      best_lr_predict_test = lr_predict_loop_test

    C_val = C_val + C_inc

best_score_C_val = C_values [recall_scores.index(best_recall_score)]
print("1st max  value of {0:.3f} ocurred at C= {1:3f}".format(best_recall_score, best_score_C_val))


# %matplotlib inline
plt.plot(C_values, recall_scores, "-")
plt.xlabel("C value")
plt.ylabel("recall score")

"""***Logistic Regression parametrization class balanced***

class_weight='balanced'
"""

# Commented out IPython magic to ensure Python compatibility.
C_start = 0.1
C_end = 5
C_inc = 0.1

C_values, recall_scores = [], []

C_val = C_start
best_recall_score = 0
while(C_val < C_end):
    C_values.append(C_val)
    lr_model_loop = LogisticRegression (C=C_val, random_state=42,solver='liblinear', class_weight='balanced' ,max_iter=1000000)
    lr_model_loop.fit(X_train, y_train.ravel())
    lr_predict_loop_test = lr_model_loop.predict(X_test)
    recall_score = metrics.recall_score(y_test, lr_predict_loop_test)
    recall_scores.append(recall_score)
    if(recall_score > best_recall_score):
      best_recall_score = recall_score
      best_lr_predict_test = lr_predict_loop_test

    C_val = C_val + C_inc

best_score_C_val = C_values [recall_scores.index(best_recall_score)]
print("1st max  value of {0:.3f} ocurred at C= {1:3f}".format(best_recall_score, best_score_C_val))


# %matplotlib inline
plt.plot(C_values, recall_scores, "-")
plt.xlabel("C value")
plt.ylabel("recall score")

lr_model = LogisticRegression(C=best_score_C_val, random_state=42, class_weight='balanced',  max_iter=1000)
lr_model.fit(X_train, y_train.ravel())
lr_predict_test = lr_model.predict(X_test)

"""Training metrics"""

#Check Accurary
print('Accurary: {0:.4f}'.format (metrics.accuracy_score(y_test, lr_predict_test)))

print("Confusion Matrix")
print("{0}".format(metrics.confusion_matrix(y_test, lr_predict_test)))
print('')
print('TN    FP')
print('FN    TP')
# TN FP
#FN  TP

print('Classification Report')
print(metrics.classification_report(y_test, lr_predict_test))

print(metrics.recall_score(y_test, lr_predict_test))

"""#### **K-fold Cross Validation**
#### Logistic Regression with Cross Validation (CV)
"""

from sklearn.linear_model import LogisticRegressionCV

lr_cv_model = LogisticRegressionCV(n_jobs=-1, random_state=42, Cs=3, cv=10, refit=True, class_weight="balanced", solver='liblinear') #set hyperparameters
lr_cv_model.fit(X_train, y_train.ravel())









"""## 7. Prediction"""

lr_cv_predict_test = lr_cv_model.predict(X_test)

"""Training metrics"""

#Check Accurary
print('Accurary: {0:.4f}'.format (metrics.accuracy_score(y_test, lr_cv_predict_test)))

print("Confusion Matrix")
print("{0}".format(metrics.confusion_matrix(y_test, lr_cv_predict_test)))
print('')
print('TN    FP')
print('FN    TP')
# TN FP
#FN  TP

print('Classification Report')
print(metrics.classification_report(y_test, lr_cv_predict_test))

print(metrics.recall_score(y_test, lr_cv_predict_test))





